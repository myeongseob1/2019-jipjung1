{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fCEDCU_qrC0"
   },
   "source": [
    "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "<h1>Welcome to Colaboratory!</h1>\n",
    "\n",
    "\n",
    "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
    "\n",
    "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 420
    },
    "colab_type": "code",
    "id": "xitplqMNk_Hc",
    "outputId": "ed4f60d2-878d-4056-c438-352dac39a112"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Introducing Colaboratory { display-mode: \"form\" }\n",
    "#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('inN8seMm7UI', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJBs_flRovLc"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
    "\n",
    "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 35
    },
    "colab_type": "code",
    "id": "gJr_9dXGpJ05",
    "outputId": "5626194c-e802-4293-942d-2908885c3c1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86400"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fhs6GZ4qFMx"
   },
   "source": [
    "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\".\n",
    "\n",
    "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 35
    },
    "colab_type": "code",
    "id": "-gE-Ez1qtyIA",
    "outputId": "8d2e4259-4682-4e19-b683-7b9087f28820"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604800"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSrWNr3MuFUS"
   },
   "source": [
    "For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Rh3-Vt9Nev9"
   },
   "source": [
    "## More Resources\n",
    "\n",
    "Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n",
    "\n",
    "### Working with Notebooks in Colaboratory\n",
    "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
    "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
    "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
    "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
    "- [Interactive forms](/notebooks/forms.ipynb)\n",
    "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
    "\n",
    "### Working with Data\n",
    "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
    "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
    "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
    "\n",
    "### Machine Learning Crash Course\n",
    "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
    "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
    "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
    "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
    "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
    "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
    "\n",
    "### Using Accelerated Hardware\n",
    "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
    "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-H6Lw1vyNNd"
   },
   "source": [
    "## Machine Learning Examples: Seedbank\n",
    "\n",
    "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n",
    "\n",
    "A few featured examples:\n",
    "\n",
    "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
    "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
    "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
    "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
    "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8mVG88wxWggi",
    "outputId": "aff7c480-61b3-4c34-9bdd-a68ba7cc6a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47500 samples, validate on 2500 samples\n",
      "Epoch 1/120\n",
      "47500/47500 [==============================] - 23s 476us/step - loss: 4.1787 - acc: 0.0730 - val_loss: 3.8045 - val_acc: 0.1288\n",
      "Epoch 2/120\n",
      "47500/47500 [==============================] - 19s 390us/step - loss: 3.8186 - acc: 0.1281 - val_loss: 3.6434 - val_acc: 0.1660\n",
      "Epoch 3/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 3.6953 - acc: 0.1479 - val_loss: 3.5697 - val_acc: 0.1676\n",
      "Epoch 4/120\n",
      "47500/47500 [==============================] - 19s 390us/step - loss: 3.6014 - acc: 0.1626 - val_loss: 3.4877 - val_acc: 0.1848\n",
      "Epoch 5/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 3.5331 - acc: 0.1728 - val_loss: 3.4341 - val_acc: 0.2004\n",
      "Epoch 6/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 3.4559 - acc: 0.1880 - val_loss: 3.3944 - val_acc: 0.2088\n",
      "Epoch 7/120\n",
      "47500/47500 [==============================] - 18s 389us/step - loss: 3.3995 - acc: 0.1942 - val_loss: 3.3593 - val_acc: 0.2128\n",
      "Epoch 8/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 3.3368 - acc: 0.2057 - val_loss: 3.3051 - val_acc: 0.2196\n",
      "Epoch 9/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 3.2859 - acc: 0.2134 - val_loss: 3.2412 - val_acc: 0.2312\n",
      "Epoch 10/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 3.2389 - acc: 0.2236 - val_loss: 3.2662 - val_acc: 0.2272\n",
      "Epoch 11/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 3.1928 - acc: 0.2275 - val_loss: 3.2316 - val_acc: 0.2320\n",
      "Epoch 12/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 3.1427 - acc: 0.2385 - val_loss: 3.1898 - val_acc: 0.2348\n",
      "Epoch 13/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 3.0996 - acc: 0.2432 - val_loss: 3.1706 - val_acc: 0.2464\n",
      "Epoch 14/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 3.0594 - acc: 0.2544 - val_loss: 3.1257 - val_acc: 0.2436\n",
      "Epoch 15/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 3.0217 - acc: 0.2610 - val_loss: 3.1608 - val_acc: 0.2420\n",
      "Epoch 16/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 2.9709 - acc: 0.2688 - val_loss: 3.1538 - val_acc: 0.2488\n",
      "Epoch 17/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 2.9351 - acc: 0.2755 - val_loss: 3.0837 - val_acc: 0.2560\n",
      "Epoch 18/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 2.9086 - acc: 0.2778 - val_loss: 3.0836 - val_acc: 0.2504\n",
      "Epoch 19/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 2.8647 - acc: 0.2880 - val_loss: 3.0618 - val_acc: 0.2596\n",
      "Epoch 20/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 2.8219 - acc: 0.2976 - val_loss: 3.0476 - val_acc: 0.2664\n",
      "Epoch 21/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 2.7861 - acc: 0.3027 - val_loss: 3.0650 - val_acc: 0.2712\n",
      "Epoch 22/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 2.7463 - acc: 0.3092 - val_loss: 3.0120 - val_acc: 0.2768\n",
      "Epoch 23/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 2.7131 - acc: 0.3161 - val_loss: 2.9822 - val_acc: 0.2748\n",
      "Epoch 24/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 2.6702 - acc: 0.3247 - val_loss: 3.0170 - val_acc: 0.2752\n",
      "Epoch 25/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 2.6446 - acc: 0.3312 - val_loss: 2.9838 - val_acc: 0.2768\n",
      "Epoch 26/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 2.6111 - acc: 0.3365 - val_loss: 2.9876 - val_acc: 0.2796\n",
      "Epoch 27/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 2.5702 - acc: 0.3446 - val_loss: 2.9739 - val_acc: 0.2812\n",
      "Epoch 28/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 2.5396 - acc: 0.3500 - val_loss: 2.9362 - val_acc: 0.2952\n",
      "Epoch 29/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 2.5000 - acc: 0.3583 - val_loss: 2.9406 - val_acc: 0.2944\n",
      "Epoch 30/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 2.4714 - acc: 0.3665 - val_loss: 2.9540 - val_acc: 0.2936\n",
      "Epoch 31/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 2.4350 - acc: 0.3714 - val_loss: 2.9509 - val_acc: 0.2848\n",
      "Epoch 32/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 2.3996 - acc: 0.3824 - val_loss: 2.9596 - val_acc: 0.2900\n",
      "Epoch 33/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 2.3621 - acc: 0.3885 - val_loss: 2.9561 - val_acc: 0.2980\n",
      "Epoch 34/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 2.3353 - acc: 0.3948 - val_loss: 2.9973 - val_acc: 0.2916\n",
      "Epoch 35/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 2.2982 - acc: 0.4023 - val_loss: 2.9505 - val_acc: 0.2964\n",
      "Epoch 36/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 2.2665 - acc: 0.4092 - val_loss: 2.9518 - val_acc: 0.2932\n",
      "Epoch 37/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 2.2410 - acc: 0.4161 - val_loss: 2.9440 - val_acc: 0.3000\n",
      "Epoch 38/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 2.2022 - acc: 0.4243 - val_loss: 2.9489 - val_acc: 0.3044\n",
      "Epoch 39/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 2.1635 - acc: 0.4320 - val_loss: 2.9249 - val_acc: 0.3056\n",
      "Epoch 40/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 2.1327 - acc: 0.4385 - val_loss: 2.9339 - val_acc: 0.3060\n",
      "Epoch 41/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 2.0940 - acc: 0.4488 - val_loss: 2.9695 - val_acc: 0.2976\n",
      "Epoch 42/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 2.0712 - acc: 0.4503 - val_loss: 2.9328 - val_acc: 0.3036\n",
      "Epoch 43/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 2.0409 - acc: 0.4602 - val_loss: 2.9238 - val_acc: 0.3132\n",
      "Epoch 44/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 2.0021 - acc: 0.4696 - val_loss: 2.9372 - val_acc: 0.3036\n",
      "Epoch 45/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.9707 - acc: 0.4757 - val_loss: 2.9483 - val_acc: 0.3064\n",
      "Epoch 46/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 1.9373 - acc: 0.4830 - val_loss: 2.9279 - val_acc: 0.3164\n",
      "Epoch 47/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.9043 - acc: 0.4916 - val_loss: 2.9458 - val_acc: 0.3088\n",
      "Epoch 48/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.8736 - acc: 0.4997 - val_loss: 2.9495 - val_acc: 0.3036\n",
      "Epoch 49/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 1.8362 - acc: 0.5074 - val_loss: 2.9513 - val_acc: 0.3080\n",
      "Epoch 50/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.8172 - acc: 0.5128 - val_loss: 2.9475 - val_acc: 0.3156\n",
      "Epoch 51/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.7802 - acc: 0.5196 - val_loss: 2.9558 - val_acc: 0.3084\n",
      "Epoch 52/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.7461 - acc: 0.5294 - val_loss: 2.9567 - val_acc: 0.3072\n",
      "Epoch 53/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 1.7106 - acc: 0.5384 - val_loss: 2.9380 - val_acc: 0.3144\n",
      "Epoch 54/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 1.6871 - acc: 0.5448 - val_loss: 2.9486 - val_acc: 0.3156\n",
      "Epoch 55/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 1.6628 - acc: 0.5476 - val_loss: 2.9738 - val_acc: 0.3152\n",
      "Epoch 56/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 1.6277 - acc: 0.5588 - val_loss: 2.9657 - val_acc: 0.3156\n",
      "Epoch 57/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 1.6004 - acc: 0.5641 - val_loss: 2.9835 - val_acc: 0.3136\n",
      "Epoch 58/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 1.5632 - acc: 0.5767 - val_loss: 2.9631 - val_acc: 0.3188\n",
      "Epoch 59/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 1.5410 - acc: 0.5836 - val_loss: 2.9709 - val_acc: 0.3236\n",
      "Epoch 60/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.5065 - acc: 0.5893 - val_loss: 2.9713 - val_acc: 0.3216\n",
      "Epoch 61/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.4740 - acc: 0.5971 - val_loss: 2.9911 - val_acc: 0.3192\n",
      "Epoch 62/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 1.4467 - acc: 0.6043 - val_loss: 2.9898 - val_acc: 0.3136\n",
      "Epoch 63/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 1.4168 - acc: 0.6120 - val_loss: 3.0152 - val_acc: 0.3156\n",
      "Epoch 64/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.3833 - acc: 0.6207 - val_loss: 2.9963 - val_acc: 0.3264\n",
      "Epoch 65/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 1.3596 - acc: 0.6281 - val_loss: 3.0120 - val_acc: 0.3180\n",
      "Epoch 66/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 1.3330 - acc: 0.6310 - val_loss: 3.0331 - val_acc: 0.3132\n",
      "Epoch 67/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 1.2995 - acc: 0.6437 - val_loss: 3.0316 - val_acc: 0.3124\n",
      "Epoch 68/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 1.2724 - acc: 0.6504 - val_loss: 3.0270 - val_acc: 0.3212\n",
      "Epoch 69/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 1.2546 - acc: 0.6528 - val_loss: 3.0638 - val_acc: 0.3132\n",
      "Epoch 70/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 1.2209 - acc: 0.6618 - val_loss: 3.0511 - val_acc: 0.3160\n",
      "Epoch 71/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 1.1948 - acc: 0.6717 - val_loss: 3.0594 - val_acc: 0.3184\n",
      "Epoch 72/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 1.1691 - acc: 0.6786 - val_loss: 3.0919 - val_acc: 0.3108\n",
      "Epoch 73/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 1.1479 - acc: 0.6834 - val_loss: 3.0811 - val_acc: 0.3156\n",
      "Epoch 74/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 1.1118 - acc: 0.6931 - val_loss: 3.0734 - val_acc: 0.3212\n",
      "Epoch 75/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 1.0848 - acc: 0.7007 - val_loss: 3.1091 - val_acc: 0.3216\n",
      "Epoch 76/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 1.0572 - acc: 0.7114 - val_loss: 3.1024 - val_acc: 0.3204\n",
      "Epoch 77/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 1.0447 - acc: 0.7105 - val_loss: 3.1227 - val_acc: 0.3240\n",
      "Epoch 78/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 1.0113 - acc: 0.7198 - val_loss: 3.1104 - val_acc: 0.3204\n",
      "Epoch 79/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 0.9912 - acc: 0.7260 - val_loss: 3.1341 - val_acc: 0.3200\n",
      "Epoch 80/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 0.9658 - acc: 0.7313 - val_loss: 3.1552 - val_acc: 0.3192\n",
      "Epoch 81/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 0.9457 - acc: 0.7413 - val_loss: 3.1467 - val_acc: 0.3184\n",
      "Epoch 82/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.9205 - acc: 0.7448 - val_loss: 3.1577 - val_acc: 0.3232\n",
      "Epoch 83/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.8923 - acc: 0.7535 - val_loss: 3.1839 - val_acc: 0.3136\n",
      "Epoch 84/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 0.8790 - acc: 0.7559 - val_loss: 3.1790 - val_acc: 0.3168\n",
      "Epoch 85/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 0.8564 - acc: 0.7614 - val_loss: 3.1745 - val_acc: 0.3240\n",
      "Epoch 86/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 0.8320 - acc: 0.7692 - val_loss: 3.2069 - val_acc: 0.3176\n",
      "Epoch 87/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 0.8115 - acc: 0.7755 - val_loss: 3.2201 - val_acc: 0.3220\n",
      "Epoch 88/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 0.7960 - acc: 0.7794 - val_loss: 3.2175 - val_acc: 0.3184\n",
      "Epoch 89/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 0.7678 - acc: 0.7897 - val_loss: 3.2454 - val_acc: 0.3196\n",
      "Epoch 90/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.7478 - acc: 0.7958 - val_loss: 3.2396 - val_acc: 0.3252\n",
      "Epoch 91/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.7288 - acc: 0.7993 - val_loss: 3.2520 - val_acc: 0.3276\n",
      "Epoch 92/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 0.7104 - acc: 0.8061 - val_loss: 3.2785 - val_acc: 0.3164\n",
      "Epoch 93/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 0.6968 - acc: 0.8080 - val_loss: 3.3050 - val_acc: 0.3176\n",
      "Epoch 94/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 0.6775 - acc: 0.8150 - val_loss: 3.3161 - val_acc: 0.3216\n",
      "Epoch 95/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 0.6581 - acc: 0.8174 - val_loss: 3.3127 - val_acc: 0.3180\n",
      "Epoch 96/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.6386 - acc: 0.8252 - val_loss: 3.3312 - val_acc: 0.3188\n",
      "Epoch 97/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 0.6276 - acc: 0.8280 - val_loss: 3.3484 - val_acc: 0.3160\n",
      "Epoch 98/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 0.6032 - acc: 0.8365 - val_loss: 3.3586 - val_acc: 0.3180\n",
      "Epoch 99/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 0.5942 - acc: 0.8361 - val_loss: 3.3735 - val_acc: 0.3176\n",
      "Epoch 100/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 0.5823 - acc: 0.8416 - val_loss: 3.3647 - val_acc: 0.3144\n",
      "Epoch 101/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.5667 - acc: 0.8441 - val_loss: 3.4002 - val_acc: 0.3160\n",
      "Epoch 102/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 0.5486 - acc: 0.8495 - val_loss: 3.3889 - val_acc: 0.3228\n",
      "Epoch 103/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.5334 - acc: 0.8555 - val_loss: 3.4312 - val_acc: 0.3180\n",
      "Epoch 104/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 0.5169 - acc: 0.8601 - val_loss: 3.4194 - val_acc: 0.3240\n",
      "Epoch 105/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.5043 - acc: 0.8628 - val_loss: 3.4433 - val_acc: 0.3180\n",
      "Epoch 106/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 0.4961 - acc: 0.8647 - val_loss: 3.4567 - val_acc: 0.3200\n",
      "Epoch 107/120\n",
      "47500/47500 [==============================] - 18s 382us/step - loss: 0.4896 - acc: 0.8680 - val_loss: 3.4629 - val_acc: 0.3144\n",
      "Epoch 108/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 0.4705 - acc: 0.8735 - val_loss: 3.4899 - val_acc: 0.3196\n",
      "Epoch 109/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 0.4612 - acc: 0.8741 - val_loss: 3.5034 - val_acc: 0.3220\n",
      "Epoch 110/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 0.4498 - acc: 0.8782 - val_loss: 3.5247 - val_acc: 0.3200\n",
      "Epoch 111/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 0.4310 - acc: 0.8831 - val_loss: 3.5264 - val_acc: 0.3168\n",
      "Epoch 112/120\n",
      "47500/47500 [==============================] - 18s 381us/step - loss: 0.4200 - acc: 0.8880 - val_loss: 3.5167 - val_acc: 0.3224\n",
      "Epoch 113/120\n",
      "47500/47500 [==============================] - 18s 387us/step - loss: 0.4143 - acc: 0.8881 - val_loss: 3.5544 - val_acc: 0.3176\n",
      "Epoch 114/120\n",
      "47500/47500 [==============================] - 18s 383us/step - loss: 0.3980 - acc: 0.8947 - val_loss: 3.5518 - val_acc: 0.3280\n",
      "Epoch 115/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 0.3957 - acc: 0.8935 - val_loss: 3.5912 - val_acc: 0.3136\n",
      "Epoch 116/120\n",
      "47500/47500 [==============================] - 18s 385us/step - loss: 0.3845 - acc: 0.8960 - val_loss: 3.6031 - val_acc: 0.3124\n",
      "Epoch 117/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 0.3822 - acc: 0.8965 - val_loss: 3.6041 - val_acc: 0.3204\n",
      "Epoch 118/120\n",
      "47500/47500 [==============================] - 18s 386us/step - loss: 0.3713 - acc: 0.8996 - val_loss: 3.6150 - val_acc: 0.3188\n",
      "Epoch 119/120\n",
      "47500/47500 [==============================] - 18s 384us/step - loss: 0.3538 - acc: 0.9049 - val_loss: 3.6379 - val_acc: 0.3120\n",
      "Epoch 120/120\n",
      "47500/47500 [==============================] - 18s 388us/step - loss: 0.3474 - acc: 0.9073 - val_loss: 3.6317 - val_acc: 0.3096\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "Test Loss and Accuracy -> [3.619549152851105, 0.32960000112652776]\n"
     ]
    }
   ],
   "source": [
    "# DNN_CIFAR-10 / MNIST보다 복잡한 데이터의 처리(R,G,B)\n",
    "import numpy as np\n",
    "from keras import datasets\n",
    "from keras.utils import np_utils\n",
    "from keras import layers, models\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class DNN(models.Sequential):\n",
    "    def __init__(self, Nin, Nh_l, Pd_l, Nout):\n",
    "        super().__init__()\n",
    "        # 첫 번째 은닉\n",
    "        self.add(layers.Dense(Nh_l[0], activation='elu',input_shape=(Nin,), name='Hidden-1'))\n",
    "        # Dropout 확률을 정한다.\n",
    "        # Dropout : 랜덤으로 몇개의 노드를 비활성화 한다.(오버피팅 방지)\n",
    "        self.add(layers.Dropout(Pd_l[0]))\n",
    "        self.add(layers.Dense(Nh_l[1], activation='elu', name='Hidden-2'))\n",
    "        self.add(layers.Dropout(Pd_l[1]))\n",
    "        self.add(layers.Dense(Nh_l[2], activation='elu', name='Hidden-3'))\n",
    "        self.add(layers.Dropout(Pd_l[2]))\n",
    "        self.add(layers.Dense(Nh_l[2], activation='elu', name='Hidden-4'))\n",
    "        self.add(layers.Dropout(Pd_l[2]))\n",
    "        self.add(layers.Dense(Nh_l[4], activation='tanh', name='Hidden-6'))\n",
    "        self.add(layers.Dropout(Pd_l[4]))\n",
    "        self.add(layers.Dense(Nout, activation='softmax'))\n",
    "        self.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.00004, beta_1=0.9, beta_2=0.999), metrics=['accuracy'])\n",
    "\n",
    "# 데이터 정리\n",
    "def Data_func():\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.cifar100.load_data()\n",
    " \n",
    "    # 0~9 사이의 정수값을 10개의 원소를 가지는 벡터로 변환\n",
    "    # 1 -> 0100000000, 2 -> 0010000000, 9 -> 0000000001\n",
    "    Y_train = np_utils.to_categorical(y_train)\n",
    "    Y_test = np_utils.to_categorical(y_test)\n",
    " \n",
    "    L, W, H, C = X_train.shape\n",
    "    X_train = X_train.reshape(-1, W * H * C)\n",
    "    X_test = X_test.reshape(-1, W * H * C)\n",
    " \n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    " \n",
    "    return (X_train, Y_train), (X_test, Y_test)\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# 모델 테스팅\n",
    "def main():\n",
    "    Nh_l = [2000, 2000, 2000,1500,1500]\n",
    "    Pd_l = [0.3, 0.2, 0.2,0.3,0.2]\n",
    "    number_of_class = 100\n",
    "    Nout = number_of_class\n",
    " \n",
    "    (X_train, Y_train), (X_test, Y_test) = Data_func()\n",
    "    model = DNN(X_train.shape[1], Nh_l, Pd_l, Nout)\n",
    "    history = model.fit(X_train, Y_train, epochs=120, batch_size=100, validation_split=0.05)\n",
    " \n",
    "    performace_test = model.evaluate(X_test, Y_test, batch_size=100)\n",
    "    print('Test Loss and Accuracy ->', performace_test)\n",
    " \n",
    "    plt.show()\n",
    "    plt.show()\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnLDZ6rq3edg"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Welcome To Colaboratory",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
