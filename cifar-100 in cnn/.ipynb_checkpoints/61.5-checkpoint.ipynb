{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fCEDCU_qrC0"
   },
   "source": [
    "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "<h1>Welcome to Colaboratory!</h1>\n",
    "\n",
    "\n",
    "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
    "\n",
    "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 420
    },
    "colab_type": "code",
    "id": "xitplqMNk_Hc",
    "outputId": "ed4f60d2-878d-4056-c438-352dac39a112"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Introducing Colaboratory { display-mode: \"form\" }\n",
    "#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('inN8seMm7UI', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJBs_flRovLc"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
    "\n",
    "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 35
    },
    "colab_type": "code",
    "id": "gJr_9dXGpJ05",
    "outputId": "5626194c-e802-4293-942d-2908885c3c1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86400"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seconds_in_a_day = 24 * 60 * 60\n",
    "seconds_in_a_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fhs6GZ4qFMx"
   },
   "source": [
    "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\".\n",
    "\n",
    "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "height": 35
    },
    "colab_type": "code",
    "id": "-gE-Ez1qtyIA",
    "outputId": "8d2e4259-4682-4e19-b683-7b9087f28820"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604800"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seconds_in_a_week = 7 * seconds_in_a_day\n",
    "seconds_in_a_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSrWNr3MuFUS"
   },
   "source": [
    "For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Rh3-Vt9Nev9"
   },
   "source": [
    "## More Resources\n",
    "\n",
    "Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n",
    "\n",
    "### Working with Notebooks in Colaboratory\n",
    "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
    "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
    "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
    "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
    "- [Interactive forms](/notebooks/forms.ipynb)\n",
    "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
    "\n",
    "### Working with Data\n",
    "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
    "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
    "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
    "\n",
    "### Machine Learning Crash Course\n",
    "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
    "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
    "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
    "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
    "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
    "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
    "\n",
    "### Using Accelerated Hardware\n",
    "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
    "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-H6Lw1vyNNd"
   },
   "source": [
    "## Machine Learning Examples: Seedbank\n",
    "\n",
    "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n",
    "\n",
    "A few featured examples:\n",
    "\n",
    "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
    "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
    "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
    "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
    "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8Nj08jhq7Ftv",
    "outputId": "e72c495d-c565-4415-c923-d8067ec1cad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               204900    \n",
      "=================================================================\n",
      "Total params: 493,700\n",
      "Trainable params: 492,804\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/125\n",
      "781/781 [==============================] - 51s 65ms/step - loss: 4.2803 - acc: 0.1249 - val_loss: 3.8506 - val_acc: 0.1906\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 3.4522 - acc: 0.2392 - val_loss: 3.1595 - val_acc: 0.2607\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 3.1686 - acc: 0.2912 - val_loss: 3.0014 - val_acc: 0.3379\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 3.1860 - acc: 0.3059 - val_loss: 2.7837 - val_acc: 0.3315\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 2.9554 - acc: 0.3418 - val_loss: 2.4269 - val_acc: 0.4106\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.8150 - acc: 0.3611 - val_loss: 2.5039 - val_acc: 0.4168\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.6732 - acc: 0.3800 - val_loss: 2.2047 - val_acc: 0.4548\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.4700 - acc: 0.4005 - val_loss: 2.3155 - val_acc: 0.4385\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.3677 - acc: 0.4218 - val_loss: 2.2042 - val_acc: 0.4567\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.3006 - acc: 0.4335 - val_loss: 2.1301 - val_acc: 0.4765\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.2525 - acc: 0.4475 - val_loss: 2.1790 - val_acc: 0.4796\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.2216 - acc: 0.4567 - val_loss: 2.1083 - val_acc: 0.4861\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.1836 - acc: 0.4667 - val_loss: 2.1163 - val_acc: 0.4851\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.1571 - acc: 0.4764 - val_loss: 2.1865 - val_acc: 0.4785\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.1351 - acc: 0.4813 - val_loss: 2.0595 - val_acc: 0.5117\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.1111 - acc: 0.4861 - val_loss: 2.1025 - val_acc: 0.5009\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.0899 - acc: 0.4910 - val_loss: 2.0269 - val_acc: 0.5144\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.0736 - acc: 0.4955 - val_loss: 1.9860 - val_acc: 0.5252\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.0648 - acc: 0.5002 - val_loss: 2.0917 - val_acc: 0.5113\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 2.0438 - acc: 0.5035 - val_loss: 1.9722 - val_acc: 0.5283\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 2.0358 - acc: 0.5071 - val_loss: 2.0678 - val_acc: 0.5146\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 2.0252 - acc: 0.5101 - val_loss: 1.9298 - val_acc: 0.5379\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 2.0093 - acc: 0.5146 - val_loss: 1.9811 - val_acc: 0.5334\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9924 - acc: 0.5191 - val_loss: 2.0049 - val_acc: 0.5294\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9919 - acc: 0.5206 - val_loss: 1.9991 - val_acc: 0.5316\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9826 - acc: 0.5223 - val_loss: 2.0071 - val_acc: 0.5296\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9716 - acc: 0.5277 - val_loss: 1.9976 - val_acc: 0.5230\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9650 - acc: 0.5264 - val_loss: 1.8859 - val_acc: 0.5540\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 1.9613 - acc: 0.5294 - val_loss: 1.9855 - val_acc: 0.5366\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9559 - acc: 0.5324 - val_loss: 2.0721 - val_acc: 0.5228\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9391 - acc: 0.5366 - val_loss: 1.9436 - val_acc: 0.5484\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9333 - acc: 0.5364 - val_loss: 1.8839 - val_acc: 0.5596\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.9234 - acc: 0.5393 - val_loss: 1.9094 - val_acc: 0.5550\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.9381 - acc: 0.5382 - val_loss: 1.9227 - val_acc: 0.5521\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9263 - acc: 0.5386 - val_loss: 1.9189 - val_acc: 0.5578\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9141 - acc: 0.5442 - val_loss: 1.9633 - val_acc: 0.5547\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9072 - acc: 0.5447 - val_loss: 2.0070 - val_acc: 0.5357\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9045 - acc: 0.5472 - val_loss: 1.9568 - val_acc: 0.5452\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.9026 - acc: 0.5471 - val_loss: 1.9475 - val_acc: 0.5534\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.8889 - acc: 0.5491 - val_loss: 1.8656 - val_acc: 0.5686\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8861 - acc: 0.5516 - val_loss: 1.8198 - val_acc: 0.5750\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 1.8849 - acc: 0.5535 - val_loss: 1.9630 - val_acc: 0.5504\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8925 - acc: 0.5514 - val_loss: 1.9089 - val_acc: 0.5584\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8740 - acc: 0.5544 - val_loss: 1.8742 - val_acc: 0.5659\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.8779 - acc: 0.5535 - val_loss: 1.8989 - val_acc: 0.5667\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8753 - acc: 0.5584 - val_loss: 1.8952 - val_acc: 0.5647\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8573 - acc: 0.5595 - val_loss: 1.9155 - val_acc: 0.5583\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8719 - acc: 0.5560 - val_loss: 1.9356 - val_acc: 0.5535\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8696 - acc: 0.5577 - val_loss: 1.9016 - val_acc: 0.5614\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8622 - acc: 0.5602 - val_loss: 1.8592 - val_acc: 0.5707\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8570 - acc: 0.5612 - val_loss: 1.9117 - val_acc: 0.5637\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8547 - acc: 0.5625 - val_loss: 1.8477 - val_acc: 0.5710\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8435 - acc: 0.5644 - val_loss: 1.8967 - val_acc: 0.5637\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.8609 - acc: 0.5607 - val_loss: 1.9155 - val_acc: 0.5612\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8435 - acc: 0.5690 - val_loss: 1.9429 - val_acc: 0.5619\n",
      "Epoch 56/125\n",
      "781/781 [==============================] - 44s 57ms/step - loss: 1.8421 - acc: 0.5658 - val_loss: 1.9051 - val_acc: 0.5661\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 44s 57ms/step - loss: 1.8508 - acc: 0.5626 - val_loss: 1.9101 - val_acc: 0.5620\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 1.8437 - acc: 0.5648 - val_loss: 1.9012 - val_acc: 0.5626\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 44s 57ms/step - loss: 1.8364 - acc: 0.5687 - val_loss: 1.8877 - val_acc: 0.5716\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 1.8342 - acc: 0.5665 - val_loss: 1.8707 - val_acc: 0.5723\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 1.8274 - acc: 0.5710 - val_loss: 1.9204 - val_acc: 0.5622\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 52s 66ms/step - loss: 1.8281 - acc: 0.5671 - val_loss: 2.0507 - val_acc: 0.5425\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 1.8357 - acc: 0.5642 - val_loss: 1.8023 - val_acc: 0.5867\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8266 - acc: 0.5667 - val_loss: 1.9340 - val_acc: 0.5629\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.8226 - acc: 0.5707 - val_loss: 1.9701 - val_acc: 0.5546\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.8261 - acc: 0.5710 - val_loss: 1.9054 - val_acc: 0.5670\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.8194 - acc: 0.5742 - val_loss: 1.8490 - val_acc: 0.5739\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8204 - acc: 0.5692 - val_loss: 1.8673 - val_acc: 0.5692\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8138 - acc: 0.5704 - val_loss: 1.9064 - val_acc: 0.5668\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.8092 - acc: 0.5727 - val_loss: 1.9033 - val_acc: 0.5660\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.8135 - acc: 0.5744 - val_loss: 1.8075 - val_acc: 0.5789\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8073 - acc: 0.5745 - val_loss: 1.8922 - val_acc: 0.5664\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 1.8123 - acc: 0.5721 - val_loss: 1.8766 - val_acc: 0.5696\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 1.8056 - acc: 0.5768 - val_loss: 1.8311 - val_acc: 0.5862\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8109 - acc: 0.5765 - val_loss: 1.8029 - val_acc: 0.5899\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.8091 - acc: 0.5754 - val_loss: 1.8771 - val_acc: 0.5747\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.7042 - acc: 0.5998 - val_loss: 1.7456 - val_acc: 0.6009\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.6681 - acc: 0.6095 - val_loss: 1.7425 - val_acc: 0.6043\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.6555 - acc: 0.6081 - val_loss: 1.7560 - val_acc: 0.6011\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.6404 - acc: 0.6132 - val_loss: 1.7727 - val_acc: 0.5973\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 45s 57ms/step - loss: 1.6389 - acc: 0.6133 - val_loss: 1.7518 - val_acc: 0.5994\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 44s 57ms/step - loss: 1.6282 - acc: 0.6126 - val_loss: 1.7628 - val_acc: 0.5962\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.6147 - acc: 0.6164 - val_loss: 1.7058 - val_acc: 0.6033\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 44s 57ms/step - loss: 1.6101 - acc: 0.6161 - val_loss: 1.7118 - val_acc: 0.6082\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 44s 56ms/step - loss: 1.6177 - acc: 0.6143 - val_loss: 1.7241 - val_acc: 0.6025\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.6101 - acc: 0.6176 - val_loss: 1.7480 - val_acc: 0.5992\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5972 - acc: 0.6194 - val_loss: 1.7800 - val_acc: 0.5918\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.6066 - acc: 0.6151 - val_loss: 1.6735 - val_acc: 0.6131\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 1.5981 - acc: 0.6201 - val_loss: 1.7460 - val_acc: 0.6005\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 47s 60ms/step - loss: 1.5834 - acc: 0.6188 - val_loss: 1.7997 - val_acc: 0.5917\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5878 - acc: 0.6201 - val_loss: 1.7340 - val_acc: 0.5960\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5875 - acc: 0.6202 - val_loss: 1.7138 - val_acc: 0.6038\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5923 - acc: 0.6183 - val_loss: 1.6945 - val_acc: 0.6044\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5804 - acc: 0.6203 - val_loss: 1.7536 - val_acc: 0.5937\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5877 - acc: 0.6171 - val_loss: 1.7289 - val_acc: 0.6005\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5759 - acc: 0.6212 - val_loss: 1.7549 - val_acc: 0.5924\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5777 - acc: 0.6203 - val_loss: 1.7485 - val_acc: 0.5932\n",
      "Epoch 98/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5663 - acc: 0.6241 - val_loss: 1.7849 - val_acc: 0.5914\n",
      "Epoch 99/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5671 - acc: 0.6219 - val_loss: 1.7160 - val_acc: 0.6014\n",
      "Epoch 100/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5772 - acc: 0.6207 - val_loss: 1.7148 - val_acc: 0.6035\n",
      "Epoch 101/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5734 - acc: 0.6212 - val_loss: 1.7581 - val_acc: 0.5969\n",
      "Epoch 102/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.5280 - acc: 0.6330 - val_loss: 1.6647 - val_acc: 0.6103\n",
      "Epoch 103/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4932 - acc: 0.6395 - val_loss: 1.6611 - val_acc: 0.6095\n",
      "Epoch 104/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4940 - acc: 0.6376 - val_loss: 1.6721 - val_acc: 0.6103\n",
      "Epoch 105/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4975 - acc: 0.6364 - val_loss: 1.6391 - val_acc: 0.6171\n",
      "Epoch 106/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4894 - acc: 0.6410 - val_loss: 1.6226 - val_acc: 0.6212\n",
      "Epoch 107/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4871 - acc: 0.6383 - val_loss: 1.6554 - val_acc: 0.6140\n",
      "Epoch 108/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4691 - acc: 0.6437 - val_loss: 1.6514 - val_acc: 0.6137\n",
      "Epoch 109/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4745 - acc: 0.6408 - val_loss: 1.6602 - val_acc: 0.6137\n",
      "Epoch 110/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4687 - acc: 0.6439 - val_loss: 1.6669 - val_acc: 0.6123\n",
      "Epoch 111/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4748 - acc: 0.6434 - val_loss: 1.6518 - val_acc: 0.6162\n",
      "Epoch 112/125\n",
      "781/781 [==============================] - 46s 60ms/step - loss: 1.4711 - acc: 0.6424 - val_loss: 1.7023 - val_acc: 0.6055\n",
      "Epoch 113/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.4642 - acc: 0.6435 - val_loss: 1.6160 - val_acc: 0.6159\n",
      "Epoch 114/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4613 - acc: 0.6455 - val_loss: 1.6223 - val_acc: 0.6203\n",
      "Epoch 115/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4728 - acc: 0.6411 - val_loss: 1.6511 - val_acc: 0.6108\n",
      "Epoch 116/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4586 - acc: 0.6442 - val_loss: 1.6567 - val_acc: 0.6140\n",
      "Epoch 117/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4545 - acc: 0.6429 - val_loss: 1.6316 - val_acc: 0.6188\n",
      "Epoch 118/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4538 - acc: 0.6434 - val_loss: 1.6579 - val_acc: 0.6123\n",
      "Epoch 119/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4542 - acc: 0.6438 - val_loss: 1.6852 - val_acc: 0.6076\n",
      "Epoch 120/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4528 - acc: 0.6443 - val_loss: 1.6317 - val_acc: 0.6175\n",
      "Epoch 121/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4524 - acc: 0.6445 - val_loss: 1.6251 - val_acc: 0.6237\n",
      "Epoch 122/125\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 1.4499 - acc: 0.6424 - val_loss: 1.6173 - val_acc: 0.6229\n",
      "Epoch 123/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.4429 - acc: 0.6463 - val_loss: 1.6737 - val_acc: 0.6095\n",
      "Epoch 124/125\n",
      "781/781 [==============================] - 46s 59ms/step - loss: 1.4404 - acc: 0.6465 - val_loss: 1.6629 - val_acc: 0.6131\n",
      "Epoch 125/125\n",
      "781/781 [==============================] - 45s 58ms/step - loss: 1.4489 - acc: 0.6452 - val_loss: 1.6477 - val_acc: 0.6150\n",
      "10000/10000 [==============================] - 1s 124us/step\n",
      "\n",
      "Test result: 61.500 loss: 1.648\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar100\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    " \n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate\n",
    " \n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 100\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    " \n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    " \n",
    "#training\n",
    "batch_size = 64\n",
    " \n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "#save to disk\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model.h5') \n",
    " \n",
    "#testing\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Welcome To Colaboratory",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
